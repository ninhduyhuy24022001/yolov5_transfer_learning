{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19235,"status":"ok","timestamp":1688041822286,"user":{"displayName":"Huy Ninh","userId":"07862882544292440665"},"user_tz":-420},"id":"kmtJ4Y5p2b0x","outputId":"3e52a653-ceab-453d-df69-62e6b4e4ddaf"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/MyDrive/DeepLearning/ComputerVision/YOLOv5_transfer\n","cats_and_dogs.yaml  download  prepare_data.ipynb  transfer.ipynb\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","%cd drive/MyDrive/DeepLearning/ComputerVision/YOLOv5_transfer\n","!ls\n","# link: https://kikaben.com/yolov5-transfer-learning-dogs-cats/"]},{"cell_type":"markdown","metadata":{"id":"nrD8nTHR23OT"},"source":["## Clone source code"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SZ-Ij-7x2kRK"},"outputs":[],"source":["!git clone https://github.com/ultralytics/yolov5\n","\n","!pip install -q -U -r yolov5/requirements.txt"]},{"cell_type":"markdown","metadata":{"id":"Zm76ZoRI4g_a"},"source":["## training"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"1BluXJWC23xH","outputId":"a0576fc2-09a1-411b-9186-7a62f2365319"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5s.pt, cfg=, data=cats_and_dogs.yaml, hyp=yolov5/data/hyps/hyp.scratch-low.yaml, epochs=10, batch_size=4, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, bucket=, cache=None, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=yolov5/runs/train, name=exp, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[10], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n","\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 âœ…\n","YOLOv5 ðŸš€ v7.0-187-g0004c74 Python-3.10.12 torch-2.0.1+cu118 CUDA:0 (Tesla T4, 15102MiB)\n","\n","\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n","\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 ðŸš€ runs in Comet\n","\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir yolov5/runs/train', view at http://localhost:6006/\n","Downloading https://ultralytics.com/assets/Arial.ttf to /root/.config/Ultralytics/Arial.ttf...\n","100% 755k/755k [00:00<00:00, 123MB/s]\n","Overriding model.yaml nc=80 with nc=2\n","\n","                 from  n    params  module                                  arguments                     \n","  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n","  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n","  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n","  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n","  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n","  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n","  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n","  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n","  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n","  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n"," 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n"," 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n"," 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n"," 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n"," 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n"," 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n"," 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n"," 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n"," 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n"," 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n"," 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n"," 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n"," 24      [17, 20, 23]  1     18879  models.yolo.Detect                      [2, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n","Model summary: 214 layers, 7025023 parameters, 7025023 gradients, 16.0 GFLOPs\n","\n","Transferred 343/349 items from yolov5s.pt\n","\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n","freezing model.0.conv.weight\n","freezing model.0.bn.weight\n","freezing model.0.bn.bias\n","freezing model.1.conv.weight\n","freezing model.1.bn.weight\n","freezing model.1.bn.bias\n","freezing model.2.cv1.conv.weight\n","freezing model.2.cv1.bn.weight\n","freezing model.2.cv1.bn.bias\n","freezing model.2.cv2.conv.weight\n","freezing model.2.cv2.bn.weight\n","freezing model.2.cv2.bn.bias\n","freezing model.2.cv3.conv.weight\n","freezing model.2.cv3.bn.weight\n","freezing model.2.cv3.bn.bias\n","freezing model.2.m.0.cv1.conv.weight\n","freezing model.2.m.0.cv1.bn.weight\n","freezing model.2.m.0.cv1.bn.bias\n","freezing model.2.m.0.cv2.conv.weight\n","freezing model.2.m.0.cv2.bn.weight\n","freezing model.2.m.0.cv2.bn.bias\n","freezing model.3.conv.weight\n","freezing model.3.bn.weight\n","freezing model.3.bn.bias\n","freezing model.4.cv1.conv.weight\n","freezing model.4.cv1.bn.weight\n","freezing model.4.cv1.bn.bias\n","freezing model.4.cv2.conv.weight\n","freezing model.4.cv2.bn.weight\n","freezing model.4.cv2.bn.bias\n","freezing model.4.cv3.conv.weight\n","freezing model.4.cv3.bn.weight\n","freezing model.4.cv3.bn.bias\n","freezing model.4.m.0.cv1.conv.weight\n","freezing model.4.m.0.cv1.bn.weight\n","freezing model.4.m.0.cv1.bn.bias\n","freezing model.4.m.0.cv2.conv.weight\n","freezing model.4.m.0.cv2.bn.weight\n","freezing model.4.m.0.cv2.bn.bias\n","freezing model.4.m.1.cv1.conv.weight\n","freezing model.4.m.1.cv1.bn.weight\n","freezing model.4.m.1.cv1.bn.bias\n","freezing model.4.m.1.cv2.conv.weight\n","freezing model.4.m.1.cv2.bn.weight\n","freezing model.4.m.1.cv2.bn.bias\n","freezing model.5.conv.weight\n","freezing model.5.bn.weight\n","freezing model.5.bn.bias\n","freezing model.6.cv1.conv.weight\n","freezing model.6.cv1.bn.weight\n","freezing model.6.cv1.bn.bias\n","freezing model.6.cv2.conv.weight\n","freezing model.6.cv2.bn.weight\n","freezing model.6.cv2.bn.bias\n","freezing model.6.cv3.conv.weight\n","freezing model.6.cv3.bn.weight\n","freezing model.6.cv3.bn.bias\n","freezing model.6.m.0.cv1.conv.weight\n","freezing model.6.m.0.cv1.bn.weight\n","freezing model.6.m.0.cv1.bn.bias\n","freezing model.6.m.0.cv2.conv.weight\n","freezing model.6.m.0.cv2.bn.weight\n","freezing model.6.m.0.cv2.bn.bias\n","freezing model.6.m.1.cv1.conv.weight\n","freezing model.6.m.1.cv1.bn.weight\n","freezing model.6.m.1.cv1.bn.bias\n","freezing model.6.m.1.cv2.conv.weight\n","freezing model.6.m.1.cv2.bn.weight\n","freezing model.6.m.1.cv2.bn.bias\n","freezing model.6.m.2.cv1.conv.weight\n","freezing model.6.m.2.cv1.bn.weight\n","freezing model.6.m.2.cv1.bn.bias\n","freezing model.6.m.2.cv2.conv.weight\n","freezing model.6.m.2.cv2.bn.weight\n","freezing model.6.m.2.cv2.bn.bias\n","freezing model.7.conv.weight\n","freezing model.7.bn.weight\n","freezing model.7.bn.bias\n","freezing model.8.cv1.conv.weight\n","freezing model.8.cv1.bn.weight\n","freezing model.8.cv1.bn.bias\n","freezing model.8.cv2.conv.weight\n","freezing model.8.cv2.bn.weight\n","freezing model.8.cv2.bn.bias\n","freezing model.8.cv3.conv.weight\n","freezing model.8.cv3.bn.weight\n","freezing model.8.cv3.bn.bias\n","freezing model.8.m.0.cv1.conv.weight\n","freezing model.8.m.0.cv1.bn.weight\n","freezing model.8.m.0.cv1.bn.bias\n","freezing model.8.m.0.cv2.conv.weight\n","freezing model.8.m.0.cv2.bn.weight\n","freezing model.8.m.0.cv2.bn.bias\n","freezing model.9.cv1.conv.weight\n","freezing model.9.cv1.bn.weight\n","freezing model.9.cv1.bn.bias\n","freezing model.9.cv2.conv.weight\n","freezing model.9.cv2.bn.weight\n","freezing model.9.cv2.bn.bias\n","\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.0005), 60 bias\n","\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n","\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/MyDrive/TGMT/yolo/data/labels/train... 412 images, 0 backgrounds, 0 corrupt:   5% 412/8093 [11:27<3:12:44,  1.51s/it]Traceback (most recent call last):\n","  File \"/content/drive/MyDrive/TGMT/yolo/yolov5/utils/dataloaders.py\", line 489, in __init__\n","    cache, exists = np.load(cache_path, allow_pickle=True).item(), True  # load dict\n","  File \"/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\", line 427, in load\n","    fid = stack.enter_context(open(os_fspath(file), \"rb\"))\n","FileNotFoundError: [Errno 2] No such file or directory: '/content/drive/MyDrive/TGMT/yolo/data/labels/train.cache'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.10/multiprocessing/pool.py\", line 856, in next\n","    item = self._items.popleft()\n","IndexError: pop from an empty deque\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/tqdm/std.py\", line 1178, in __iter__\n","    for obj in iterable:\n","  File \"/usr/lib/python3.10/multiprocessing/pool.py\", line 861, in next\n","    self._cond.wait(timeout)\n","  File \"/usr/lib/python3.10/threading.py\", line 320, in wait\n","    waiter.acquire()\n","KeyboardInterrupt\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/content/drive/MyDrive/TGMT/yolo/yolov5/train.py\", line 647, in <module>\n","    main(opt)\n","  File \"/content/drive/MyDrive/TGMT/yolo/yolov5/train.py\", line 536, in main\n","    train(opt.hyp, opt, device, callbacks)\n","  File \"/content/drive/MyDrive/TGMT/yolo/yolov5/train.py\", line 195, in train\n","    train_loader, dataset = create_dataloader(train_path,\n","  File \"/content/drive/MyDrive/TGMT/yolo/yolov5/utils/dataloaders.py\", line 124, in create_dataloader\n","    dataset = LoadImagesAndLabels(\n","  File \"/content/drive/MyDrive/TGMT/yolo/yolov5/utils/dataloaders.py\", line 493, in __init__\n","    cache, exists = self.cache_labels(cache_path, prefix), False  # run cache ops\n","  File \"/content/drive/MyDrive/TGMT/yolo/yolov5/utils/dataloaders.py\", line 617, in cache_labels\n","    for im_file, lb, shape, segments, nm_f, nf_f, ne_f, nc_f, msg in pbar:\n","  File \"/usr/local/lib/python3.10/dist-packages/tqdm/std.py\", line 1193, in __iter__\n","    self.close()\n","  File \"/usr/local/lib/python3.10/dist-packages/tqdm/std.py\", line 1272, in close\n","    self._decr_instances(self)\n","  File \"/usr/local/lib/python3.10/dist-packages/tqdm/std.py\", line 584, in _decr_instances\n","    with cls._lock:\n","  File \"/usr/local/lib/python3.10/dist-packages/tqdm/std.py\", line 110, in __enter__\n","    self.acquire()\n","  File \"/usr/local/lib/python3.10/dist-packages/tqdm/std.py\", line 103, in acquire\n","    lock.acquire(*a, **k)\n","KeyboardInterrupt\n","^C\n"]}],"source":["!python yolov5/train.py --data cats_and_dogs.yaml --weights yolov5s.pt --epochs 10 --batch 32 --freeze 10 --optimizer Adam"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21998,"status":"ok","timestamp":1687932973395,"user":{"displayName":"Huy Ninh","userId":"16681526256730359977"},"user_tz":-420},"id":"5_HJwShH_mSM","outputId":"04f7c0ea-de13-4e28-9ce3-76f5cb439336"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[34m\u001b[1mval: \u001b[0mdata=cats_and_dogs.yaml, weights=['yolov5/runs/train/exp2/weights/best.pt'], batch_size=32, imgsz=640, conf_thres=0.001, iou_thres=0.6, max_det=300, task=test, device=, workers=8, single_cls=False, augment=False, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=False, project=yolov5/runs/val, name=exp, exist_ok=False, half=False, dnn=False\n","YOLOv5 ðŸš€ v7.0-187-g0004c74 Python-3.10.12 torch-2.0.1+cu118 CUDA:0 (Tesla T4, 15102MiB)\n","\n","Fusing layers... \n","Model summary: 157 layers, 7015519 parameters, 0 gradients, 15.8 GFLOPs\n","\u001b[34m\u001b[1mtest: \u001b[0mScanning /content/drive/MyDrive/TGMT/yolo/data/labels/test.cache... 99 images, 0 backgrounds, 0 corrupt: 100% 99/99 [00:00<?, ?it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:05<00:00,  1.38s/it]\n","                   all         99        114      0.727      0.807      0.839      0.595\n","                   cat         99         55      0.695        0.8      0.824      0.571\n","                   dog         99         59      0.758      0.814      0.854      0.618\n","Speed: 0.2ms pre-process, 12.1ms inference, 3.2ms NMS per image at shape (32, 3, 640, 640)\n","Results saved to \u001b[1myolov5/runs/val/exp7\u001b[0m\n"]}],"source":["!python yolov5/val.py --data cats_and_dogs.yaml --weights yolov5/runs/train/exp2/weights/best.pt --task test"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21139,"status":"ok","timestamp":1687933023341,"user":{"displayName":"Huy Ninh","userId":"16681526256730359977"},"user_tz":-420},"id":"QWqPJAVQNcea","outputId":"0334a499-b653-4b78-9786-0c821a8ac512"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[34m\u001b[1mval: \u001b[0mdata=cats_and_dogs.yaml, weights=['yolov5/runs/train/exp3/weights/best.pt'], batch_size=32, imgsz=640, conf_thres=0.001, iou_thres=0.6, max_det=300, task=val, device=, workers=8, single_cls=False, augment=False, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=False, project=yolov5/runs/val, name=exp, exist_ok=False, half=False, dnn=False\n","YOLOv5 ðŸš€ v7.0-187-g0004c74 Python-3.10.12 torch-2.0.1+cu118 CUDA:0 (Tesla T4, 15102MiB)\n","\n","Fusing layers... \n","Model summary: 157 layers, 7015519 parameters, 0 gradients, 15.8 GFLOPs\n","\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/TGMT/yolo/data/labels/val.cache... 99 images, 0 backgrounds, 0 corrupt: 100% 99/99 [00:00<?, ?it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:05<00:00,  1.39s/it]\n","                   all         99        116       0.78      0.906      0.889      0.638\n","                   cat         99         58      0.852      0.892      0.925      0.639\n","                   dog         99         58      0.708       0.92      0.853      0.638\n","Speed: 0.2ms pre-process, 9.0ms inference, 3.1ms NMS per image at shape (32, 3, 640, 640)\n","Results saved to \u001b[1myolov5/runs/val/exp9\u001b[0m\n"]}],"source":["!python yolov5/val.py --data cats_and_dogs.yaml --weights yolov5/runs/train/exp3/weights/best.pt"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13166,"status":"ok","timestamp":1687933054461,"user":{"displayName":"Huy Ninh","userId":"16681526256730359977"},"user_tz":-420},"id":"zwsagN3FKgbh","outputId":"064cbccc-6059-4aad-c877-3f3a05483f4e"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[34m\u001b[1mdetect: \u001b[0mweights=['yolov5/runs/train/exp3/weights/best.pt'], source=data/images/test/0a7c9a14600849bd.jpg, data=yolov5/data/coco128.yaml, imgsz=[640, 640], conf_thres=0.25, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=yolov5/runs/detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False, vid_stride=1\n","YOLOv5 ðŸš€ v7.0-187-g0004c74 Python-3.10.12 torch-2.0.1+cu118 CUDA:0 (Tesla T4, 15102MiB)\n","\n","Fusing layers... \n","Model summary: 157 layers, 7015519 parameters, 0 gradients, 15.8 GFLOPs\n","image 1/1 /content/drive/MyDrive/TGMT/yolo/data/images/test/0a7c9a14600849bd.jpg: 480x640 1 dog, 45.9ms\n","Speed: 0.5ms pre-process, 45.9ms inference, 1.7ms NMS per image at shape (1, 3, 640, 640)\n","Results saved to \u001b[1myolov5/runs/detect/exp2\u001b[0m\n"]}],"source":["!python yolov5/detect.py --weights yolov5/runs/train/exp3/weights/best.pt --img 640 --conf 0.25 --source data/images/test/0a7c9a14600849bd.jpg"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}