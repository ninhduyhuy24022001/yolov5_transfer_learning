{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18581,"status":"ok","timestamp":1688110158444,"user":{"displayName":"Huy Ninh","userId":"07043958495325832514"},"user_tz":-420},"id":"kmtJ4Y5p2b0x","outputId":"e759ce3a-8fe4-4f4c-84a7-837f63188cd7"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n","/content/drive/MyDrive/DeepLearning/ComputerVision/yolo_transfer\n","cats_and_dogs.yaml  download\t\tsample_test  transfer.ipynb  yolov5s.pt\n","data\t\t    prepare_data.ipynb\ttest.ipynb   yolov5\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","%cd drive/MyDrive/DeepLearning/ComputerVision/yolo_transfer\n","!ls\n","# link: https://kikaben.com/yolov5-transfer-learning-dogs-cats/"]},{"cell_type":"markdown","metadata":{"id":"nrD8nTHR23OT"},"source":["## Clone source code"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":33839,"status":"ok","timestamp":1688110192277,"user":{"displayName":"Huy Ninh","userId":"07043958495325832514"},"user_tz":-420},"id":"SZ-Ij-7x2kRK","outputId":"d3368e40-be51-40dd-ea65-82a04d365ec0"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m184.3/184.3 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m93.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m109.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m62.6/62.6 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m36.3/36.3 MB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m612.6/612.6 kB\u001b[0m \u001b[31m57.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m89.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m804.0/804.0 kB\u001b[0m \u001b[31m59.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m341.8/341.8 kB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","ipython 7.34.0 requires jedi>=0.16, which is not installed.\n","google-colab 1.0.0 requires pandas==1.5.3, but you have pandas 2.0.3 which is incompatible.\n","google-colab 1.0.0 requires requests==2.27.1, but you have requests 2.31.0 which is incompatible.\n","numba 0.56.4 requires numpy<1.24,>=1.18, but you have numpy 1.25.0 which is incompatible.\n","tensorflow 2.12.0 requires numpy<1.24,>=1.22, but you have numpy 1.25.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m"]}],"source":["# !git clone https://github.com/ultralytics/yolov5\n","\n","!pip install -q -U -r yolov5/requirements.txt"]},{"cell_type":"markdown","metadata":{"id":"Zm76ZoRI4g_a"},"source":["## training"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5610024,"status":"ok","timestamp":1688075329546,"user":{"displayName":"Huy Ninh","userId":"07043958495325832514"},"user_tz":-420},"id":"1BluXJWC23xH","outputId":"57734b8b-c4ec-4c97-8fa5-fb578ab030cd"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5s.pt, cfg=, data=cats_and_dogs.yaml, hyp=yolov5/data/hyps/hyp.scratch-low.yaml, epochs=20, batch_size=16, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, bucket=, cache=None, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=yolov5/runs/train, name=exp, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[10], save_period=2, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n","\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 ‚úÖ\n","YOLOv5 üöÄ v7.0-187-g0004c74 Python-3.10.12 torch-2.0.1+cu118 CUDA:0 (Tesla T4, 15102MiB)\n","\n","\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n","\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 üöÄ runs in Comet\n","\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir yolov5/runs/train', view at http://localhost:6006/\n","Overriding model.yaml nc=80 with nc=2\n","\n","                 from  n    params  module                                  arguments                     \n","  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n","  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n","  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n","  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n","  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n","  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n","  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n","  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n","  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n","  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n"," 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n"," 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n"," 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n"," 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n"," 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n"," 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n"," 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n"," 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n"," 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n"," 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n"," 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n"," 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n"," 24      [17, 20, 23]  1     18879  models.yolo.Detect                      [2, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n","Model summary: 214 layers, 7025023 parameters, 7025023 gradients, 16.0 GFLOPs\n","\n","Transferred 343/349 items from yolov5s.pt\n","\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ‚úÖ\n","freezing model.0.conv.weight\n","freezing model.0.bn.weight\n","freezing model.0.bn.bias\n","freezing model.1.conv.weight\n","freezing model.1.bn.weight\n","freezing model.1.bn.bias\n","freezing model.2.cv1.conv.weight\n","freezing model.2.cv1.bn.weight\n","freezing model.2.cv1.bn.bias\n","freezing model.2.cv2.conv.weight\n","freezing model.2.cv2.bn.weight\n","freezing model.2.cv2.bn.bias\n","freezing model.2.cv3.conv.weight\n","freezing model.2.cv3.bn.weight\n","freezing model.2.cv3.bn.bias\n","freezing model.2.m.0.cv1.conv.weight\n","freezing model.2.m.0.cv1.bn.weight\n","freezing model.2.m.0.cv1.bn.bias\n","freezing model.2.m.0.cv2.conv.weight\n","freezing model.2.m.0.cv2.bn.weight\n","freezing model.2.m.0.cv2.bn.bias\n","freezing model.3.conv.weight\n","freezing model.3.bn.weight\n","freezing model.3.bn.bias\n","freezing model.4.cv1.conv.weight\n","freezing model.4.cv1.bn.weight\n","freezing model.4.cv1.bn.bias\n","freezing model.4.cv2.conv.weight\n","freezing model.4.cv2.bn.weight\n","freezing model.4.cv2.bn.bias\n","freezing model.4.cv3.conv.weight\n","freezing model.4.cv3.bn.weight\n","freezing model.4.cv3.bn.bias\n","freezing model.4.m.0.cv1.conv.weight\n","freezing model.4.m.0.cv1.bn.weight\n","freezing model.4.m.0.cv1.bn.bias\n","freezing model.4.m.0.cv2.conv.weight\n","freezing model.4.m.0.cv2.bn.weight\n","freezing model.4.m.0.cv2.bn.bias\n","freezing model.4.m.1.cv1.conv.weight\n","freezing model.4.m.1.cv1.bn.weight\n","freezing model.4.m.1.cv1.bn.bias\n","freezing model.4.m.1.cv2.conv.weight\n","freezing model.4.m.1.cv2.bn.weight\n","freezing model.4.m.1.cv2.bn.bias\n","freezing model.5.conv.weight\n","freezing model.5.bn.weight\n","freezing model.5.bn.bias\n","freezing model.6.cv1.conv.weight\n","freezing model.6.cv1.bn.weight\n","freezing model.6.cv1.bn.bias\n","freezing model.6.cv2.conv.weight\n","freezing model.6.cv2.bn.weight\n","freezing model.6.cv2.bn.bias\n","freezing model.6.cv3.conv.weight\n","freezing model.6.cv3.bn.weight\n","freezing model.6.cv3.bn.bias\n","freezing model.6.m.0.cv1.conv.weight\n","freezing model.6.m.0.cv1.bn.weight\n","freezing model.6.m.0.cv1.bn.bias\n","freezing model.6.m.0.cv2.conv.weight\n","freezing model.6.m.0.cv2.bn.weight\n","freezing model.6.m.0.cv2.bn.bias\n","freezing model.6.m.1.cv1.conv.weight\n","freezing model.6.m.1.cv1.bn.weight\n","freezing model.6.m.1.cv1.bn.bias\n","freezing model.6.m.1.cv2.conv.weight\n","freezing model.6.m.1.cv2.bn.weight\n","freezing model.6.m.1.cv2.bn.bias\n","freezing model.6.m.2.cv1.conv.weight\n","freezing model.6.m.2.cv1.bn.weight\n","freezing model.6.m.2.cv1.bn.bias\n","freezing model.6.m.2.cv2.conv.weight\n","freezing model.6.m.2.cv2.bn.weight\n","freezing model.6.m.2.cv2.bn.bias\n","freezing model.7.conv.weight\n","freezing model.7.bn.weight\n","freezing model.7.bn.bias\n","freezing model.8.cv1.conv.weight\n","freezing model.8.cv1.bn.weight\n","freezing model.8.cv1.bn.bias\n","freezing model.8.cv2.conv.weight\n","freezing model.8.cv2.bn.weight\n","freezing model.8.cv2.bn.bias\n","freezing model.8.cv3.conv.weight\n","freezing model.8.cv3.bn.weight\n","freezing model.8.cv3.bn.bias\n","freezing model.8.m.0.cv1.conv.weight\n","freezing model.8.m.0.cv1.bn.weight\n","freezing model.8.m.0.cv1.bn.bias\n","freezing model.8.m.0.cv2.conv.weight\n","freezing model.8.m.0.cv2.bn.weight\n","freezing model.8.m.0.cv2.bn.bias\n","freezing model.9.cv1.conv.weight\n","freezing model.9.cv1.bn.weight\n","freezing model.9.cv1.bn.bias\n","freezing model.9.cv2.conv.weight\n","freezing model.9.cv2.bn.weight\n","freezing model.9.cv2.bn.bias\n","\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.0005), 60 bias\n","\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n","\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/MyDrive/DeepLearning/ComputerVision/yolo_transfer/data/labels/train.cache... 3176 images, 0 backgrounds, 0 corrupt: 100% 3176/3176 [00:00<?, ?it/s]\n","\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/DeepLearning/ComputerVision/yolo_transfer/data/labels/val.cache... 396 images, 0 backgrounds, 0 corrupt: 100% 396/396 [00:00<?, ?it/s]\n","\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0m3.80 anchors/target, 1.000 Best Possible Recall (BPR). Current anchors are a good fit to dataset ‚úÖ\n","Plotting labels to yolov5/runs/train/exp6/labels.jpg... \n","Image sizes 640 train, 640 val\n","Using 2 dataloader workers\n","Logging results to \u001b[1myolov5/runs/train/exp6\u001b[0m\n","Starting training for 20 epochs...\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","       0/19      1.38G    0.06966    0.02517    0.02379         14        640: 100% 199/199 [13:38<00:00,  4.11s/it]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 13/13 [00:15<00:00,  1.19s/it]\n","                   all        396        463      0.563      0.643      0.629        0.3\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","       1/19      2.68G    0.04694     0.0186    0.01524         24        640: 100% 199/199 [03:48<00:00,  1.15s/it]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 13/13 [00:12<00:00,  1.02it/s]\n","                   all        396        463      0.659      0.748      0.751      0.447\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","       2/19      2.68G    0.04243    0.01712    0.01359         17        640: 100% 199/199 [03:50<00:00,  1.16s/it]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 13/13 [00:11<00:00,  1.15it/s]\n","                   all        396        463      0.549      0.743      0.646      0.338\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","       3/19      2.68G    0.03813    0.01686    0.01327         16        640: 100% 199/199 [03:59<00:00,  1.20s/it]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 13/13 [00:12<00:00,  1.08it/s]\n","                   all        396        463      0.661      0.779      0.752      0.449\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","       4/19      2.68G    0.03524    0.01658    0.01261         20        640: 100% 199/199 [03:53<00:00,  1.17s/it]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 13/13 [00:13<00:00,  1.06s/it]\n","                   all        396        463      0.713      0.841      0.813      0.535\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","       5/19      2.68G    0.03261    0.01624    0.01198         25        640: 100% 199/199 [03:49<00:00,  1.15s/it]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 13/13 [00:14<00:00,  1.15s/it]\n","                   all        396        463      0.715      0.819      0.814      0.557\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","       6/19      2.68G    0.03083    0.01626    0.01202         19        640: 100% 199/199 [03:56<00:00,  1.19s/it]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 13/13 [00:13<00:00,  1.05s/it]\n","                   all        396        463      0.744      0.816      0.819      0.558\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","       7/19      2.68G     0.0294    0.01568    0.01077         23        640: 100% 199/199 [03:53<00:00,  1.17s/it]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 13/13 [00:12<00:00,  1.01it/s]\n","                   all        396        463      0.692      0.843      0.823      0.585\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","       8/19      2.68G    0.02952    0.01578    0.01097         18        640: 100% 199/199 [03:56<00:00,  1.19s/it]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 13/13 [00:10<00:00,  1.21it/s]\n","                   all        396        463      0.715      0.842      0.831        0.6\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","       9/19      2.68G    0.02848    0.01535   0.009915         22        640: 100% 199/199 [03:54<00:00,  1.18s/it]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 13/13 [00:11<00:00,  1.11it/s]\n","                   all        396        463      0.735      0.795      0.813      0.592\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","      10/19      2.68G    0.02704    0.01529   0.009699         20        640: 100% 199/199 [03:51<00:00,  1.16s/it]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 13/13 [00:10<00:00,  1.20it/s]\n","                   all        396        463      0.716       0.85      0.822      0.597\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","      11/19      2.68G    0.02673    0.01521   0.009419         22        640: 100% 199/199 [03:48<00:00,  1.15s/it]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 13/13 [00:14<00:00,  1.14s/it]\n","                   all        396        463      0.747      0.825      0.819      0.599\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","      12/19      2.68G    0.02628    0.01524   0.009675          9        640: 100% 199/199 [04:01<00:00,  1.21s/it]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 13/13 [00:14<00:00,  1.13s/it]\n","                   all        396        463      0.762      0.787       0.84      0.626\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","      13/19      2.68G    0.02554    0.01498   0.009676         27        640: 100% 199/199 [03:58<00:00,  1.20s/it]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 13/13 [00:12<00:00,  1.03it/s]\n","                   all        396        463      0.717      0.819      0.827      0.618\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","      14/19      2.68G    0.02554     0.0149   0.009157         16        640: 100% 199/199 [03:57<00:00,  1.19s/it]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 13/13 [00:13<00:00,  1.04s/it]\n","                   all        396        463      0.779      0.786      0.835      0.622\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","      15/19      2.68G    0.02462    0.01452   0.008891         30        640: 100% 199/199 [03:55<00:00,  1.18s/it]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 13/13 [00:12<00:00,  1.03it/s]\n","                   all        396        463      0.731      0.825      0.824      0.617\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","      16/19      2.68G    0.02416    0.01445   0.008247         18        640: 100% 199/199 [03:56<00:00,  1.19s/it]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 13/13 [00:12<00:00,  1.01it/s]\n","                   all        396        463      0.733      0.832      0.836       0.63\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","      17/19      2.68G    0.02422    0.01446   0.008644         21        640: 100% 199/199 [03:50<00:00,  1.16s/it]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 13/13 [00:13<00:00,  1.01s/it]\n","                   all        396        463      0.724      0.853      0.839      0.642\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","      18/19      2.68G    0.02319    0.01435   0.007763         13        640: 100% 199/199 [03:52<00:00,  1.17s/it]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 13/13 [00:14<00:00,  1.09s/it]\n","                   all        396        463      0.764      0.797      0.839      0.637\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","      19/19      2.68G    0.02352    0.01457    0.00706         30        640: 100% 199/199 [03:50<00:00,  1.16s/it]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 13/13 [00:14<00:00,  1.10s/it]\n","                   all        396        463      0.766      0.808      0.844      0.646\n","\n","20 epochs completed in 1.543 hours.\n","Optimizer stripped from yolov5/runs/train/exp6/weights/last.pt, 14.4MB\n","Optimizer stripped from yolov5/runs/train/exp6/weights/best.pt, 14.4MB\n","\n","Validating yolov5/runs/train/exp6/weights/best.pt...\n","Fusing layers... \n","Model summary: 157 layers, 7015519 parameters, 0 gradients, 15.8 GFLOPs\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 13/13 [00:13<00:00,  1.03s/it]\n","                   all        396        463      0.766      0.806      0.844      0.647\n","                   cat        396        218      0.774        0.8      0.846      0.638\n","                   dog        396        245      0.758      0.812      0.841      0.656\n","Results saved to \u001b[1myolov5/runs/train/exp6\u001b[0m\n"]}],"source":["# transfer v·ªõi 20 epochs, bathsize 16 ƒë·ªëng bƒÉng backbone v√† save checkpoint m·ªói 2 epochs\n","\n","!python yolov5/train.py --data cats_and_dogs.yaml --weights yolov5s.pt --epochs 20 --batch 16 --freeze 10 --save 2"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":37455,"status":"ok","timestamp":1688075403595,"user":{"displayName":"Huy Ninh","userId":"07043958495325832514"},"user_tz":-420},"id":"5_HJwShH_mSM","outputId":"d0f393c3-53d1-4777-eb7a-a8d50baccda1"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[34m\u001b[1mval: \u001b[0mdata=cats_and_dogs.yaml, weights=['yolov5/runs/train/exp6/weights/best.pt'], batch_size=32, imgsz=640, conf_thres=0.001, iou_thres=0.6, max_det=300, task=test, device=, workers=8, single_cls=False, augment=False, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=False, project=yolov5/runs/val, name=exp, exist_ok=False, half=False, dnn=False\n","YOLOv5 üöÄ v7.0-187-g0004c74 Python-3.10.12 torch-2.0.1+cu118 CUDA:0 (Tesla T4, 15102MiB)\n","\n","Fusing layers... \n","Model summary: 157 layers, 7015519 parameters, 0 gradients, 15.8 GFLOPs\n","\u001b[34m\u001b[1mtest: \u001b[0mScanning /content/drive/MyDrive/DeepLearning/ComputerVision/yolo_transfer/data/labels/test.cache... 398 images, 0 backgrounds, 0 corrupt: 100% 398/398 [00:00<?, ?it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 13/13 [00:19<00:00,  1.47s/it]\n","                   all        398        441      0.747      0.858      0.855      0.651\n","                   cat        398        206      0.792      0.879      0.888      0.659\n","                   dog        398        235      0.702      0.838      0.821      0.643\n","Speed: 0.2ms pre-process, 6.2ms inference, 2.9ms NMS per image at shape (32, 3, 640, 640)\n","Results saved to \u001b[1myolov5/runs/val/exp5\u001b[0m\n"]}],"source":["# ƒê√°nh gi√° m√¥ h√¨nh tr√™n t·∫≠p test\n","\n","!python yolov5/val.py --data cats_and_dogs.yaml --weights yolov5/runs/train/exp6/weights/best.pt --task test"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":37925,"status":"ok","timestamp":1688080266222,"user":{"displayName":"Huy Ninh","userId":"07043958495325832514"},"user_tz":-420},"id":"QWqPJAVQNcea","outputId":"c1ea6483-b5de-49e6-de49-96744ec8ca45"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[34m\u001b[1mval: \u001b[0mdata=cats_and_dogs.yaml, weights=['yolov5s.pt'], batch_size=32, imgsz=640, conf_thres=0.001, iou_thres=0.6, max_det=300, task=test, device=, workers=8, single_cls=False, augment=False, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=False, project=yolov5/runs/val, name=exp, exist_ok=False, half=False, dnn=False\n","YOLOv5 üöÄ v7.0-187-g0004c74 Python-3.10.12 torch-2.0.1+cu118 CUDA:0 (Tesla T4, 15102MiB)\n","\n","Fusing layers... \n","YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients\n","Downloading https://ultralytics.com/assets/Arial.ttf to /root/.config/Ultralytics/Arial.ttf...\n","100% 755k/755k [00:00<00:00, 14.8MB/s]\n","Traceback (most recent call last):\n","  File \"/content/drive/MyDrive/DeepLearning/ComputerVision/yolo_transfer/yolov5/val.py\", line 409, in <module>\n","    main(opt)\n","  File \"/content/drive/MyDrive/DeepLearning/ComputerVision/yolo_transfer/yolov5/val.py\", line 380, in main\n","    run(**vars(opt))\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n","    return func(*args, **kwargs)\n","  File \"/content/drive/MyDrive/DeepLearning/ComputerVision/yolo_transfer/yolov5/val.py\", line 170, in run\n","    assert ncm == nc, f'{weights} ({ncm} classes) trained on different --data than what you passed ({nc} ' \\\n","AssertionError: ['yolov5s.pt'] (80 classes) trained on different --data than what you passed (2 classes). Pass correct combination of --weights and --data that are trained together.\n"]}],"source":["# ƒê√°nh gi√° m√¥ h√¨nh tr√™n t·∫≠p validate\n","\n","!python yolov5/val.py --data cats_and_dogs.yaml --weights yolov5s.pt --task test"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":45884,"status":"ok","timestamp":1688110239244,"user":{"displayName":"Huy Ninh","userId":"07043958495325832514"},"user_tz":-420},"id":"zwsagN3FKgbh","outputId":"d52ba64c-2bb5-4fd3-93ab-2aa8ba10dc71"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[34m\u001b[1mdetect: \u001b[0mweights=['yolov5s.pt'], source=sample_test, data=yolov5/data/coco128.yaml, imgsz=[640, 640], conf_thres=0.25, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=yolov5/runs/detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False, vid_stride=1\n","YOLOv5 üöÄ v7.0-187-g0004c74 Python-3.10.12 torch-2.0.1+cu118 CUDA:0 (Tesla T4, 15102MiB)\n","\n","Fusing layers... \n","YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients\n","image 1/4 /content/drive/MyDrive/DeepLearning/ComputerVision/yolo_transfer/sample_test/cat.jpg: 416x640 1 cat, 72.2ms\n","image 2/4 /content/drive/MyDrive/DeepLearning/ComputerVision/yolo_transfer/sample_test/dog.jpg: 640x512 2 bears, 70.6ms\n","image 3/4 /content/drive/MyDrive/DeepLearning/ComputerVision/yolo_transfer/sample_test/dog_1.jpg: 640x640 1 dog, 12.4ms\n","image 4/4 /content/drive/MyDrive/DeepLearning/ComputerVision/yolo_transfer/sample_test/dog_cat.jpg: 448x640 1 cat, 1 dog, 1 bed, 44.3ms\n","Speed: 0.6ms pre-process, 49.9ms inference, 11.7ms NMS per image at shape (1, 3, 640, 640)\n","Results saved to \u001b[1myolov5/runs/detect/exp9\u001b[0m\n"]}],"source":["# Ch·∫°y k·∫øt qu·∫£ h√¨nh tr√™n ·∫£nh.\n","\n","!python yolov5/detect.py --weights yolov5s.pt --img 640 --conf 0.25 --source sample_test"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
